{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c90fc0d-7883-4833-b4c8-a2aa5a07322c",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "1. **Load the Data**  \n",
    "2. **Check and Drop Columns with Too Many Missing Values**  \n",
    "3. **Remove Rows Missing the Target Variable**  \n",
    "4. **Fill Remaining Missing Values**  \n",
    "5. **Remove Outliers (Simple Approach)**  \n",
    "6. **Encode Categorical Variables (Two Methods)**  \n",
    "7. **Transform the Target to Reduce Skew**  \n",
    "8. **Check Correlation**  \n",
    "9. **Scale Features (Demonstration)**  \n",
    "10. **Final Preview**\n",
    "\n",
    "---\n",
    "\n",
    "Will use the **“AmesHousing.csv”** dataset and treat **\"SalePrice\"** as the target for a linear regression model.  \n",
    "If you have a different dataset, simply adjust the file path and target column accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3657f8-e199-4a32-8fa0-fdadd459cd6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## STEP 1: Imports, File Path, and Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b41e96-eeb1-4171-b1e2-9e96879169e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import (\n",
    "    PowerTransformer, \n",
    "    StandardScaler, \n",
    "    MinMaxScaler, \n",
    "    PolynomialFeatures\n",
    ")\n",
    "\n",
    "# Our dataset location\n",
    "file_path = \"AmesHousing.csv\"\n",
    "# The column we want to predict\n",
    "target_col = \"SalePrice\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd9056b-0a33-4300-8a24-5064c260bafc",
   "metadata": {},
   "source": [
    "## Why These Imports?\n",
    "\n",
    "- **pandas**: For data handling  \n",
    "- **numpy**: For numeric calculations  \n",
    "- **matplotlib/seaborn**: For plotting and data visualization  \n",
    "- **sklearn.preprocessing**: For transformations (scaling and power transforms)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d055b3d-1ea8-4e5a-94e9-aa405e336bda",
   "metadata": {},
   "source": [
    "## STEP 2: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca853cd-99c9-4c52-b1d1-f27aabf7a145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the CSV into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Show the first 5 rows (to see what the data looks like)\n",
    "print(\"Data Loaded Successfully!\\n\")\n",
    "print(df.head())\n",
    "\n",
    "# Shape tells us how many rows and columns\n",
    "print(\"\\nShape of the dataset:\", df.shape)\n",
    "\n",
    "# info() shows column names, data types, and where missing values might be\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e79e6a-ff07-4761-af9b-8322948b6c07",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "- `df.head()`: Lets us see the top rows to get a feel for what columns we have.  \n",
    "- `df.info()`: Shows how many non-missing (non-null) entries each column has.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a380c7b-3b87-4112-a918-0cd8d04ce51b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## From the Output:\n",
    "\n",
    "### Missing Values:\n",
    "- **Alley** has only 198 non-null entries out of 2930, meaning most houses don’t have this data (over 2,700 missing).  \n",
    "- Similarly, **Pool QC** has just 13 non-null entries, etc.\n",
    "\n",
    "### Data Types:\n",
    "- **int64**: Usually represents whole numbers (e.g., `Year Built`, `Overall Qual`).  \n",
    "- **float64**: Often represents numeric data that can have decimals (e.g., `Lot Frontage`).  \n",
    "- **object**: Typically represents text or categorical data (e.g., `Street`, `Neighborhood`).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3b6706-be8d-465d-a639-4d90a78088d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## STEP 3: Drop Columns with Too Many Missing Values\n",
    "\n",
    "In a real dataset, some columns may have lots of missing data.  \n",
    "For DEMO purposes, we will drop any column if more than **30%** of its entries are missing.  \n",
    "You can choose a different threshold based on your needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7bd55-5890-4fb5-87fb-d5ab3df9f4d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_high_missing_columns(dataframe, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Drop any column that has a fraction of missing values \n",
    "    greater than the given threshold (default=0.3).\n",
    "    \n",
    "    For example, 0.3 means 30% of that column's values are missing.\n",
    "    \"\"\"\n",
    "    # fraction of missing values for each column\n",
    "    missing_fraction = dataframe.isnull().mean()\n",
    "    \n",
    "    # which columns are above the threshold\n",
    "    cols_to_drop = missing_fraction[missing_fraction > threshold].index\n",
    "    print(f\"Columns to drop (>{threshold*100}% missing):\", list(cols_to_drop))\n",
    "    \n",
    "    # drop them in place\n",
    "    dataframe.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "df = drop_high_missing_columns(df, threshold=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd0c30a-49a9-4941-a90b-83d8e681fed3",
   "metadata": {},
   "source": [
    "## STEP 4: Drop Rows with Missing Target\n",
    "\n",
    "We cannot train a model if our target (**\"SalePrice\"**) is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59751af7-45be-47f0-8c48-86a1d18d33cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_missing_target(dataframe, target_column):\n",
    "    \"\"\"\n",
    "    Drop rows that have missing values for the target column.\n",
    "    \"\"\"\n",
    "    if target_column not in dataframe.columns:\n",
    "        print(f\"Target column '{target_column}' not found. Skipping.\")\n",
    "        return dataframe\n",
    "    \n",
    "    before = len(dataframe)\n",
    "    dataframe.dropna(subset=[target_column], inplace=True)\n",
    "    after = len(dataframe)\n",
    "    print(f\"Dropped {before - after} rows that had missing {target_column}.\")\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "df = drop_missing_target(df, target_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f491a7d5-7364-4a14-9327-cf0523d65123",
   "metadata": {},
   "source": [
    "## STEP 5: Fill Remaining Missing Values\n",
    "\n",
    "Now we decide how to fill in the other missing values. A simple approach:  \n",
    "\n",
    "- **Numeric columns**: Fill with the median value.  \n",
    "- **Categorical (string/object) columns**: Fill with `\"Missing\"`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbee6fc-983a-4fe8-ba22-dcc48f7c4b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fill_missing_values(dataframe):\n",
    "    \"\"\"\n",
    "    Fills missing numeric values with the median,\n",
    "    and missing categorical values with 'Missing'.\n",
    "    \"\"\"\n",
    "    numeric_cols = dataframe.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = dataframe.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "    # Numeric -> median\n",
    "    for col in numeric_cols:\n",
    "        if dataframe[col].isnull().any():\n",
    "            dataframe[col].fillna(dataframe[col].median(), inplace=True)\n",
    "\n",
    "    # Categorical -> \"Missing\"\n",
    "    for col in categorical_cols:\n",
    "        if dataframe[col].isnull().any():\n",
    "            dataframe[col].fillna(\"Missing\", inplace=True)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1c4bb5-daad-460e-8e32-8ad1d25976b8",
   "metadata": {},
   "source": [
    "### Check Missing Values Before:\n",
    "- Let us see how many missing values exist in total.  \n",
    "- We will also inspect an example column (**Garage Cond**) that was previously missing in many rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917190c-3d5a-4886-a2cd-ccf85dbda28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count total missing values across the entire DataFrame\n",
    "missing_before = df.isnull().sum().sum()\n",
    "print(f\"Total missing values (BEFORE): {missing_before}\")\n",
    "\n",
    "# Example: Check the unique values in 'Alley' column (if it exists)\n",
    "if 'Garage Cond' in df.columns:\n",
    "    print(\"\\nUnique values in 'Garage Cond' BEFORE filling:\")\n",
    "    print(df['Garage Cond'].unique()[:10])  # Just show up to 10 unique values\n",
    "else:\n",
    "    print(\"\\n'Garage Cond' column not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3955f788-00d3-435a-aaa9-9dbe2bb03776",
   "metadata": {},
   "source": [
    "### Fill Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e5024-90a3-462b-bfa6-95ea07351f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = fill_missing_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5753e9-4a40-402e-9f18-b6ccd5f0e385",
   "metadata": {},
   "source": [
    "### Check Missing Values After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83920f76-c8b1-41ba-89b1-a04eaedd6d2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count total missing values across the entire DataFrame again\n",
    "missing_after = df.isnull().sum().sum()\n",
    "print(f\"\\nTotal missing values (AFTER): {missing_after}\")\n",
    "\n",
    "# Check the 'Alley' column again\n",
    "if 'Garage Cond' in df.columns:\n",
    "    print(\"\\nUnique values in 'Garage Cond' AFTER filling:\")\n",
    "    print(df['Garage Cond'].unique()[:10])\n",
    "else:\n",
    "    print(\"\\n'Garage Cond' column not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfbc74f-b81f-42c2-a689-ae80e3bc9f6f",
   "metadata": {},
   "source": [
    "## STEP 6: Remove Outliers\n",
    "\n",
    "For DEMO purposes, let us remove houses where **\"Gr Liv Area\"** (above-ground living area) is greater than **4000 square feet**.  \n",
    "These are rare and can skew linear models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ec9a8-15f5-4140-9a8f-cfae0e4c16b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_outliers(dataframe, col_name=\"Gr Liv Area\", upper_limit=4000):\n",
    "    \"\"\"\n",
    "    Remove any row where 'Gr Liv Area' is above 4000 sq ft (simple example).\n",
    "    We'll also show a before/after boxplot of this column.\n",
    "    \"\"\"\n",
    "    if col_name not in dataframe.columns:\n",
    "        print(f\"Column '{col_name}' not found. Skipping outlier removal.\")\n",
    "        return dataframe\n",
    "    \n",
    "    df_before = dataframe.copy()\n",
    "    \n",
    "    # Boxplot before\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(x=df_before[col_name])\n",
    "    plt.title(f\"{col_name} - Before\")\n",
    "    \n",
    "    # Actually remove the outliers\n",
    "    dataframe = dataframe[dataframe[col_name] < upper_limit]\n",
    "    \n",
    "    # Boxplot after\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=dataframe[col_name])\n",
    "    plt.title(f\"{col_name} - After\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "df = remove_outliers(df, col_name=\"Gr Liv Area\", upper_limit=4000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0356978c-e781-4873-8ed3-8f428aa2e42f",
   "metadata": {},
   "source": [
    "# Boxplots: Before and After Removing Outliers\n",
    "\n",
    "## 1. Boxplot Before Removing Outliers (Left Plot)\n",
    "The left boxplot shows the distribution of \"Gr Liv Area\" values prior to outlier removal.\n",
    "\n",
    "- Several data points exceed 4,000 square feet, identified as outliers (shown as circles outside the whiskers).\n",
    "- These extreme values could distort analyses such as linear regression by introducing skewness or bias.\n",
    "\n",
    "## 2. Boxplot After Removing Outliers (Right Plot)\n",
    "The right boxplot shows the same data after filtering out values greater than 4,000 square feet.\n",
    "\n",
    "## 3. Key Takeaways:\n",
    "- Removing extreme outliers minimizes the risk of skewness or bias, especially when the outliers are rare or atypical.\n",
    "- This adjustment improves the reliability of statistical methods, like regression, by ensuring the analysis reflects the core patterns of the dataset without undue influence from non-representative data points.\n",
    "- By refining the dataset in this way, models are more likely to produce accurate and meaningful insights.\n",
    "\n",
    "## Note\n",
    "\n",
    "In reality, outlier handling depends on context.  \n",
    "Here, we are removing large houses simply as an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7466b0c5-e283-409b-8468-5316d232a336",
   "metadata": {},
   "source": [
    "## STEP 7: Encode Categorical Variables\n",
    "\n",
    "Machine learning models like linear regression typically prefer numeric data.  \n",
    "So we must convert (“encode”) any categorical columns.  \n",
    "\n",
    "- **Frequency Encoding**:  \n",
    "  For columns with many categories (over 10), we replace each category with how often it appears in the dataset (a fraction from 0 to 1).  \n",
    "\n",
    "- **One-Hot Encoding**:  \n",
    "  For columns with few categories, we create new columns of 0s and 1s for each category (minus one category to avoid redundancies).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c37a0-8a73-4f09-8ced-0a71e9f73eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_categorical_features(dataframe, freq_threshold=10):\n",
    "    \"\"\"\n",
    "    If a column has more than `freq_threshold` unique categories,\n",
    "    we use frequency encoding (replacing each category with its fraction of occurrences).\n",
    "    Otherwise, we do one-hot encoding (creating new 0/1 columns for each category).\n",
    "\n",
    "    This version prints out:\n",
    "      1) Which columns were frequency-encoded vs. one-hot-encoded\n",
    "      2) A small sample (first 5 rows) of one frequency-encoded column\n",
    "         and one one-hot-encoded column (if any exist).\n",
    "    \"\"\"\n",
    "    # Identify categorical columns (object dtype)\n",
    "    cat_cols = dataframe.select_dtypes(include=[\"object\"]).columns\n",
    "    \n",
    "    one_hot_frames = []\n",
    "    freq_frames = {}\n",
    "    \n",
    "    # Keep track of columns that were encoded by which method\n",
    "    freq_encoded_columns = []\n",
    "    one_hot_encoded_columns = []\n",
    "    \n",
    "    # Decide how to encode each categorical column\n",
    "    for col in cat_cols:\n",
    "        unique_count = dataframe[col].nunique()\n",
    "        \n",
    "        if unique_count > freq_threshold:\n",
    "            # Frequency encoding\n",
    "            freq_map = dataframe[col].value_counts(normalize=True)\n",
    "            freq_frames[col + \"_freq\"] = dataframe[col].map(freq_map)\n",
    "            freq_encoded_columns.append(col)\n",
    "        else:\n",
    "            # One-hot encoding\n",
    "            dummies = pd.get_dummies(dataframe[col], prefix=col, drop_first=True)\n",
    "            one_hot_frames.append(dummies)\n",
    "            one_hot_encoded_columns.append(col)\n",
    "\n",
    "    # Merge frequency-encoded columns back\n",
    "    if freq_frames:\n",
    "        freq_df = pd.DataFrame(freq_frames, index=dataframe.index)\n",
    "        dataframe = pd.concat([dataframe, freq_df], axis=1)\n",
    "\n",
    "    # Merge one-hot-encoded columns back\n",
    "    if one_hot_frames:\n",
    "        ohe_df = pd.concat(one_hot_frames, axis=1)\n",
    "        dataframe = pd.concat([dataframe, ohe_df], axis=1)\n",
    "\n",
    "    # Drop original categorical columns (replaced by numeric ones)\n",
    "    dataframe.drop(columns=cat_cols, inplace=True)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # PRINT A SUMMARY OF RESULTS\n",
    "    # ---------------------------\n",
    "    print(\"\\nEncoding Summary:\")\n",
    "\n",
    "    # Frequency-encoded columns\n",
    "    if freq_encoded_columns:\n",
    "        print(f\"  Frequency-encoded columns (>{freq_threshold} unique categories):\")\n",
    "        for col in freq_encoded_columns:\n",
    "            print(f\"    - {col}\")\n",
    "    else:\n",
    "        print(f\"  No columns had more than {freq_threshold} unique categories.\")\n",
    "\n",
    "    # One-hot-encoded columns\n",
    "    if one_hot_encoded_columns:\n",
    "        print(f\"\\n  One-hot-encoded columns (<= {freq_threshold} unique categories):\")\n",
    "        for col in one_hot_encoded_columns:\n",
    "            print(f\"    - {col}\")\n",
    "    else:\n",
    "        print(f\"  No columns had {freq_threshold} or fewer unique categories.\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # SHOW EXAMPLES OF TRANSFORMED COLUMNS\n",
    "    # ---------------------------\n",
    "    # 1) Example of a frequency-encoded column\n",
    "    if freq_encoded_columns:\n",
    "        freq_example = freq_encoded_columns[0]               # just pick the first column we freq-encoded\n",
    "        freq_example_col = freq_example + \"_freq\"            # this is how we named it above\n",
    "        if freq_example_col in dataframe.columns:\n",
    "            print(f\"\\nExample of frequency-encoded column: '{freq_example_col}'\")\n",
    "            display(dataframe[[freq_example_col]].head(5))\n",
    "    \n",
    "    # 2) Example of a one-hot-encoded column\n",
    "    if one_hot_encoded_columns:\n",
    "        oh_example = one_hot_encoded_columns[0]              # pick the first column we one-hot-encoded\n",
    "        # Our new one-hot columns for this feature start with oh_example + \"_\"\n",
    "        oh_cols = [c for c in dataframe.columns if c.startswith(oh_example + \"_\")]\n",
    "        if oh_cols:\n",
    "            print(f\"\\nExample of one-hot-encoded columns from '{oh_example}': {oh_cols}\")\n",
    "            display(dataframe[oh_cols].head(5))\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Usage Example\n",
    "df = encode_categorical_features(df, freq_threshold=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae259ac7-ff01-4590-a386-0827918bb83e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## STEP 8: Transform the Target to Reduce Skew\n",
    "\n",
    "Often, house prices or other monetary values are not evenly distributed (they can be heavily skewed).  \n",
    "Linear regression works better if the target is more normally distributed.  \n",
    "\n",
    "We will use **PowerTransformer** (Yeo-Johnson method) to help normalize it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1272a74d-a6fc-4de6-9b90-49081ee42f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_target_skew(dataframe, target_column=\"SalePrice\"):\n",
    "    \"\"\"\n",
    "    Applies the PowerTransformer to the target column \n",
    "    to reduce skew and make the data more normal-like.\n",
    "    \"\"\"\n",
    "    if target_column not in dataframe.columns:\n",
    "        print(f\"Target column '{target_column}' not found. Skipping.\")\n",
    "        return dataframe\n",
    "    \n",
    "    # Before transformation\n",
    "    target_before = dataframe[target_column].copy()\n",
    "    skew_before = skew(target_before)\n",
    "    print(f\"Initial {target_column} skew: {skew_before:.2f}\")\n",
    "\n",
    "    # Apply the Yeo-Johnson transform\n",
    "    transformer = PowerTransformer(method=\"yeo-johnson\")\n",
    "    dataframe[[target_column]] = transformer.fit_transform(dataframe[[target_column]])\n",
    "\n",
    "    # After transformation\n",
    "    target_after = dataframe[target_column].copy()\n",
    "    skew_after = skew(target_after)\n",
    "    print(f\"Transformed {target_column} skew: {skew_after:.2f}\")\n",
    "\n",
    "    # Compare the distributions\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(target_before, kde=True)\n",
    "    plt.title(f\"{target_column} - Before Transform\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(target_after, kde=True)\n",
    "    plt.title(f\"{target_column} - After Transform\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "df = transform_target_skew(df, target_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55f12a-4bf2-41b9-b273-95530106221d",
   "metadata": {},
   "source": [
    "# Interpretation of SalePrice Transformation\n",
    "\n",
    "## Before Transformation (Left Plot)\n",
    "The histogram shows the **SalePrice** distribution before applying the PowerTransformer.\n",
    "\n",
    "- The distribution is **right-skewed**, with a long tail stretching toward higher values.\n",
    "- Most house prices are clustered at the lower end, with fewer high-priced houses.\n",
    "- This skewness can negatively impact regression models by introducing bias and violating assumptions of normality.\n",
    "\n",
    "\n",
    "## After Transformation (Right Plot)\n",
    "The histogram shows the **SalePrice** distribution after applying the PowerTransformer (Yeo-Johnson method).\n",
    "\n",
    "- The transformed distribution is now **approximately normal**, with values symmetrically centered around the mean.\n",
    "- The extreme right tail has been compressed, reducing the skewness to nearly zero.\n",
    "- This normalization ensures the data better aligns with the assumptions of statistical and machine learning models, improving model accuracy and interpretability.\n",
    "\n",
    "## Overall Impact:\n",
    "- **Before Transformation:** The skewness (1.59) indicates a heavily distorted distribution.\n",
    "- **After Transformation:** The skewness is reduced to 0.00, signifying a near-normal distribution.\n",
    "- **Takeaway:** The transformation helps the dataset meet the requirements of regression models and improves the reliability of predictions by removing distortions caused by skewed target data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7bff10-3e08-42a5-ae3e-130cf35da96e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## STEP 9: Correlation Analysis\n",
    "\n",
    "Correlation helps us see how strongly each feature is related to our target.  \n",
    "We also display a heatmap of all correlations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d0d0e-7776-4d10-b8ff-7268b18f7f29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def correlation_analysis(dataframe, target_column=\"SalePrice\"):\n",
    "    \"\"\"\n",
    "    Prints out the correlation of each column with the target,\n",
    "    and then shows a full correlation heatmap.\n",
    "    \"\"\"\n",
    "    if target_column not in dataframe.columns:\n",
    "        print(f\"Target column '{target_column}' not found. Skipping correlation analysis.\")\n",
    "        return\n",
    "\n",
    "    corr_matrix = dataframe.corr(numeric_only=True)  # for pandas 1.5+\n",
    "    target_corr = corr_matrix[target_column].sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nTop 10 features MOST positively correlated with target:\")\n",
    "    print(target_corr[1:11])  # skip the target itself at index 0\n",
    "\n",
    "    print(\"\\nTop 10 features MOST negatively correlated with target:\")\n",
    "    print(target_corr[-10:])\n",
    "\n",
    "    # Plot a heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, cmap=\"coolwarm\", annot=False, square=True)\n",
    "    plt.title(\"Correlation Matrix Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "correlation_analysis(df, target_column=target_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6394f9ae-b805-4363-a970-bf3046ce0a21",
   "metadata": {},
   "source": [
    "## STEP 10: Demonstrate Feature Scaling\n",
    "\n",
    "Scaling features using **StandardScaler** can sometimes help linear regression,  \n",
    "especially if the ranges of features differ greatly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76727bb2-1c90-4ac6-8917-de14cd9dba8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_features_demo(dataframe, target_column=\"SalePrice\", col_to_plot=\"Gr Liv Area\"):\n",
    "    \"\"\"\n",
    "    Shows how one column looks before and after StandardScaler.\n",
    "    \"\"\"\n",
    "    # Separate the features from the target, if the target column exists\n",
    "    if target_column in dataframe.columns:\n",
    "        X = dataframe.drop(columns=[target_column])\n",
    "    else:\n",
    "        X = dataframe.copy()\n",
    "\n",
    "    # Fit the StandardScaler\n",
    "    scaler_std = StandardScaler()\n",
    "    X_std = scaler_std.fit_transform(X)\n",
    "    \n",
    "    # Convert to a DataFrame for easy plotting\n",
    "    X_std_df = pd.DataFrame(X_std, columns=X.columns, index=X.index)\n",
    "\n",
    "    # Plot original vs. standard-scaled distributions for col_to_plot\n",
    "    if col_to_plot in X.columns:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        # Original\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(X[col_to_plot], kde=True, color=\"skyblue\")\n",
    "        plt.title(f\"{col_to_plot} - Original\")\n",
    "\n",
    "        # StandardScaled\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.histplot(X_std_df[col_to_plot], kde=True, color=\"orange\")\n",
    "        plt.title(f\"{col_to_plot} - StandardScaled\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Column '{col_to_plot}' was not found. Cannot plot distribution.\")\n",
    "        \n",
    "scale_features_demo(df, target_column=\"SalePrice\", col_to_plot=\"Gr Liv Area\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f80ed-ef12-4392-b8fe-b518d8f0fbb1",
   "metadata": {},
   "source": [
    "## STEP 12: Final Preview\n",
    "\n",
    "Let us see how our DataFrame looks now and what its final shape is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea9a58d-d44e-4f81-a189-15756d75f53f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nPreview of the processed DataFrame (unscaled):\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nFinal shape of processed data:\", df.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
